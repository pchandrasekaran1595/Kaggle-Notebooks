{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb46af8c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-11-01T11:49:31.815027Z",
     "iopub.status.busy": "2022-11-01T11:49:31.813999Z",
     "iopub.status.idle": "2022-11-01T11:50:05.629020Z",
     "shell.execute_reply": "2022-11-01T11:50:05.627840Z"
    },
    "papermill": {
     "duration": 33.82655,
     "end_time": "2022-11-01T11:50:05.633907",
     "exception": false,
     "start_time": "2022-11-01T11:49:31.807357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install timm -q\n",
    "pip install onnx -q\n",
    "pip install onnxruntime -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "936ffcfb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-01T11:50:05.643284Z",
     "iopub.status.busy": "2022-11-01T11:50:05.642826Z",
     "iopub.status.idle": "2022-11-01T11:50:09.525245Z",
     "shell.execute_reply": "2022-11-01T11:50:09.523839Z"
    },
    "papermill": {
     "duration": 3.890714,
     "end_time": "2022-11-01T11:50:09.528274",
     "exception": false,
     "start_time": "2022-11-01T11:50:05.637560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import timm\n",
    "import onnx\n",
    "import torch\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f1d1d6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-01T11:50:09.538010Z",
     "iopub.status.busy": "2022-11-01T11:50:09.537288Z",
     "iopub.status.idle": "2022-11-01T11:50:09.543512Z",
     "shell.execute_reply": "2022-11-01T11:50:09.542287Z"
    },
    "papermill": {
     "duration": 0.013677,
     "end_time": "2022-11-01T11:50:09.545842",
     "exception": false,
     "start_time": "2022-11-01T11:50:09.532165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUT_PATH: str = \"../input/fdsimages\"\n",
    "MODEL_PATH: str = \"../input/fds-en4encoder-onnx\"\n",
    "\n",
    "REFERENCE_IMAGE_NAMES: list = [\n",
    "    \"Image_1_1.jpg\",\n",
    "    \"Image_2_1.jpg\",\n",
    "    \"Image_3_1.jpg\",\n",
    "    \"Image_4_1.jpg\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dbf7980",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-01T11:50:09.554863Z",
     "iopub.status.busy": "2022-11-01T11:50:09.554444Z",
     "iopub.status.idle": "2022-11-01T11:50:09.564992Z",
     "shell.execute_reply": "2022-11-01T11:50:09.563424Z"
    },
    "papermill": {
     "duration": 0.018161,
     "end_time": "2022-11-01T11:50:09.567685",
     "exception": false,
     "start_time": "2022-11-01T11:50:09.549524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def breaker() -> None:\n",
    "    print(\"\\n\" + 50*\"*\" + \"\\n\")\n",
    "\n",
    "\n",
    "def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float: \n",
    "    return np.dot(a, b.reshape(-1, 1)) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "\n",
    "def get_image(path: str) -> np.ndarray: \n",
    "    return cv2.cvtColor(src=cv2.imread(path, cv2.IMREAD_COLOR), code=cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "def get_reference_embeddings(models) -> list:\n",
    "    reference_embeddings: list = []\n",
    "    \n",
    "    for i in range(len(models)):\n",
    "        reference_image = get_image(os.path.join(INPUT_PATH, REFERENCE_IMAGE_NAMES[i]))\n",
    "        reference_embeddings.append(models[i].infer(reference_image))\n",
    "    \n",
    "    return reference_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abc79c1",
   "metadata": {
    "papermill": {
     "duration": 0.003219,
     "end_time": "2022-11-01T11:50:09.574612",
     "exception": false,
     "start_time": "2022-11-01T11:50:09.571393",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **ONNX**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65b75192",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-01T11:50:09.584344Z",
     "iopub.status.busy": "2022-11-01T11:50:09.583121Z",
     "iopub.status.idle": "2022-11-01T11:50:09.598133Z",
     "shell.execute_reply": "2022-11-01T11:50:09.597200Z"
    },
    "papermill": {
     "duration": 0.022422,
     "end_time": "2022-11-01T11:50:09.600665",
     "exception": false,
     "start_time": "2022-11-01T11:50:09.578243",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, model_type: int=1):\n",
    "        self.ort_session = None\n",
    "        self.size: int = 384\n",
    "\n",
    "        if model_type == 1:\n",
    "            self.path = os.path.join(MODEL_PATH, \"onnx/I1T1-EN384AE.onnx\")\n",
    "            self.mean = [0.28106, 0.31696, 0.30282]\n",
    "            self.std = [0.26783, 0.27980, 0.27595]\n",
    "        \n",
    "        elif model_type == 2:\n",
    "            self.path = os.path.join(MODEL_PATH, \"onnx/I2T1-EN384AE.onnx\")\n",
    "            self.mean = [0.33171, 0.40140, 0.42093]\n",
    "            self.std = [0.23583, 0.24294, 0.24042]\n",
    "        \n",
    "        elif model_type == 3:\n",
    "            self.path = os.path.join(MODEL_PATH, \"onnx/I3T1-EN384AE.onnx\")\n",
    "            self.mean = [0.35717, 0.39790, 0.37881]\n",
    "            self.std = [0.23648, 0.24145, 0.23653]\n",
    "        \n",
    "        elif model_type == 4:\n",
    "            self.path = os.path.join(MODEL_PATH, \"onnx/I4T1-EN384AE.onnx\")\n",
    "            self.mean = [0.41605, 0.47578, 0.45165]\n",
    "            self.std = [0.20690, 0.20804, 0.19967]\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Incorrect 'model_type'\")\n",
    "\n",
    "    def setup(self):\n",
    "        model = onnx.load(self.path)\n",
    "        onnx.checker.check_model(model)\n",
    "        self.ort_session = ort.InferenceSession(self.path)\n",
    "\n",
    "    def infer(self, image: np.ndarray) -> np.ndarray:\n",
    "        image = image / 255\n",
    "        image = cv2.resize(src=image, dsize=(self.size, self.size), interpolation=cv2.INTER_CUBIC).transpose(2, 0, 1)\n",
    "        for i in range(image.shape[0]):\n",
    "            image[i, :, :] = (image[i, :, :] - self.mean[i]) / self.std[i]\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        input = {self.ort_session.get_inputs()[0].name : image.astype(\"float32\")}\n",
    "        result = self.ort_session.run(None, input)\n",
    "        return result[0].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b107ca0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-01T11:50:09.609618Z",
     "iopub.status.busy": "2022-11-01T11:50:09.608944Z",
     "iopub.status.idle": "2022-11-01T11:50:30.041234Z",
     "shell.execute_reply": "2022-11-01T11:50:30.039817Z"
    },
    "papermill": {
     "duration": 20.439972,
     "end_time": "2022-11-01T11:50:30.044127",
     "exception": false,
     "start_time": "2022-11-01T11:50:09.604155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**************************************************\n",
      "\n",
      "Image_1_1.jpg - CS1: 1.00, CS2: 0.63, CS3: 0.49, CS4: 0.33\n",
      "Image_1_2.jpg - CS1: 0.80, CS2: 0.69, CS3: 0.49, CS4: 0.50\n",
      "Image_1_3.jpg - CS1: 0.89, CS2: 0.54, CS3: 0.42, CS4: 0.19\n",
      "Image_1_4.jpg - CS1: 0.79, CS2: 0.62, CS3: 0.45, CS4: 0.40\n",
      "Image_2_1.jpg - CS1: 0.66, CS2: 1.00, CS3: 0.45, CS4: 0.39\n",
      "Image_2_2.jpg - CS1: 0.67, CS2: 0.85, CS3: 0.43, CS4: 0.42\n",
      "Image_2_3.jpg - CS1: 0.58, CS2: 0.63, CS3: 0.37, CS4: 0.32\n",
      "Image_2_4.jpg - CS1: 0.55, CS2: 0.76, CS3: 0.41, CS4: 0.35\n",
      "Image_3_1.jpg - CS1: 0.33, CS2: 0.35, CS3: 1.00, CS4: 0.42\n",
      "Image_3_2.jpg - CS1: 0.33, CS2: 0.26, CS3: 0.43, CS4: 0.11\n",
      "Image_3_3.jpg - CS1: 0.30, CS2: 0.15, CS3: 0.53, CS4: 0.13\n",
      "Image_3_4.jpg - CS1: 0.29, CS2: 0.20, CS3: 0.54, CS4: 0.05\n",
      "Image_4_1.jpg - CS1: 0.36, CS2: 0.49, CS3: 0.20, CS4: 1.00\n",
      "Image_4_2.jpg - CS1: 0.41, CS2: 0.51, CS3: 0.23, CS4: 0.75\n",
      "Image_4_3.jpg - CS1: 0.35, CS2: 0.40, CS3: 0.23, CS4: 0.78\n",
      "Image_4_4.jpg - CS1: 0.39, CS2: 0.45, CS3: 0.24, CS4: 0.79\n",
      "\n",
      "**************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "onnx_models: list = [\n",
    "    Model(model_type=1), \n",
    "    Model(model_type=2), \n",
    "    Model(model_type=3), \n",
    "    Model(model_type=4)\n",
    "]\n",
    "for model in onnx_models: model.setup()\n",
    "reference_embeddings = get_reference_embeddings(onnx_models)\n",
    "\n",
    "filenames = sorted(os.listdir(INPUT_PATH))\n",
    "\n",
    "breaker()\n",
    "for filename in filenames:\n",
    "    embeddings: list = []\n",
    "    css: list = []\n",
    "    for model in onnx_models:\n",
    "        image = get_image(os.path.join(INPUT_PATH, filename))\n",
    "        embeddings.append(model.infer(image))\n",
    "    for i in range(len(onnx_models)): css.append(cosine_similarity(reference_embeddings[i], embeddings[i])[0])\n",
    "    print(f\"{filename} - CS1: {css[0]:.2f}, CS2: {css[1]:.2f}, CS3: {css[2]:.2f}, CS4: {css[3]:.2f}\")\n",
    "breaker()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4fefaf",
   "metadata": {
    "papermill": {
     "duration": 0.005173,
     "end_time": "2022-11-01T11:50:30.054879",
     "exception": false,
     "start_time": "2022-11-01T11:50:30.049706",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **PT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a936ef76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-01T11:50:30.068415Z",
     "iopub.status.busy": "2022-11-01T11:50:30.067310Z",
     "iopub.status.idle": "2022-11-01T11:50:30.079376Z",
     "shell.execute_reply": "2022-11-01T11:50:30.078049Z"
    },
    "papermill": {
     "duration": 0.021636,
     "end_time": "2022-11-01T11:50:30.082036",
     "exception": false,
     "start_time": "2022-11-01T11:50:30.060400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, model_type: int=1):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.encoder = timm.create_model(\"efficientnet_b4\", pretrained=False)\n",
    "        self.encoder = torch.nn.Sequential(*[*self.encoder.children()][:-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    \n",
    "def get_features(model, path, transform, image):\n",
    "    model.load_state_dict(torch.load(path, map_location=torch.device(\"cpu\")))\n",
    "    model.to(torch.device(\"cpu\"))\n",
    "    model.eval()\n",
    "    \n",
    "    image = cv2.resize(src=image, dsize=(384, 384), interpolation=cv2.INTER_AREA)\n",
    "    with torch.no_grad(): embeddings = model(transform(image).to(torch.device(\"cpu\")).unsqueeze(dim=0))\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58367b87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-01T11:50:30.093855Z",
     "iopub.status.busy": "2022-11-01T11:50:30.093349Z",
     "iopub.status.idle": "2022-11-01T11:51:04.336843Z",
     "shell.execute_reply": "2022-11-01T11:51:04.335464Z"
    },
    "papermill": {
     "duration": 34.253676,
     "end_time": "2022-11-01T11:51:04.340555",
     "exception": false,
     "start_time": "2022-11-01T11:50:30.086879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**************************************************\n",
      "\n",
      "Image_1_1.jpg - CS1: 1.00, CS2: 0.63, CS3: 0.48, CS4: 0.33\n",
      "Image_1_2.jpg - CS1: 0.80, CS2: 0.69, CS3: 0.49, CS4: 0.51\n",
      "Image_1_3.jpg - CS1: 0.89, CS2: 0.54, CS3: 0.42, CS4: 0.19\n",
      "Image_1_4.jpg - CS1: 0.79, CS2: 0.62, CS3: 0.46, CS4: 0.40\n",
      "Image_2_1.jpg - CS1: 0.66, CS2: 1.00, CS3: 0.45, CS4: 0.38\n",
      "Image_2_2.jpg - CS1: 0.67, CS2: 0.85, CS3: 0.43, CS4: 0.42\n",
      "Image_2_3.jpg - CS1: 0.58, CS2: 0.63, CS3: 0.37, CS4: 0.32\n",
      "Image_2_4.jpg - CS1: 0.55, CS2: 0.76, CS3: 0.41, CS4: 0.35\n",
      "Image_3_1.jpg - CS1: 0.34, CS2: 0.35, CS3: 1.00, CS4: 0.42\n",
      "Image_3_2.jpg - CS1: 0.33, CS2: 0.26, CS3: 0.43, CS4: 0.11\n",
      "Image_3_3.jpg - CS1: 0.30, CS2: 0.15, CS3: 0.53, CS4: 0.13\n",
      "Image_3_4.jpg - CS1: 0.29, CS2: 0.19, CS3: 0.54, CS4: 0.05\n",
      "Image_4_1.jpg - CS1: 0.37, CS2: 0.50, CS3: 0.20, CS4: 1.00\n",
      "Image_4_2.jpg - CS1: 0.41, CS2: 0.51, CS3: 0.22, CS4: 0.75\n",
      "Image_4_3.jpg - CS1: 0.35, CS2: 0.40, CS3: 0.23, CS4: 0.78\n",
      "Image_4_4.jpg - CS1: 0.39, CS2: 0.45, CS3: 0.24, CS4: 0.78\n",
      "\n",
      "**************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reference_embeddings: list = []\n",
    "    \n",
    "model = Model().to(torch.device(\"cpu\"))\n",
    "paths = [\n",
    "    \"pt/I1T1-EN384AE.pt\",\n",
    "    \"pt/I2T1-EN384AE.pt\",\n",
    "    \"pt/I3T1-EN384AE.pt\",\n",
    "    \"pt/I4T1-EN384AE.pt\",\n",
    "]\n",
    "image_transforms = [\n",
    "    transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.28106, 0.31696, 0.30282], [0.26783, 0.27980, 0.27595]),\n",
    "    ]),\n",
    "    transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.33171, 0.40140, 0.42093], [0.23583, 0.24294, 0.24042]),\n",
    "    ]),\n",
    "    transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.35717, 0.39790, 0.37881], [0.23648, 0.24145, 0.23653]),\n",
    "    ]),\n",
    "    transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.41605, 0.47578, 0.45165], [0.20690, 0.20804, 0.19967]),\n",
    "    ]),\n",
    "]\n",
    "\n",
    "for i in range(len(paths)):\n",
    "    image = get_image(os.path.join(INPUT_PATH, REFERENCE_IMAGE_NAMES[i]))\n",
    "    reference_embeddings.append(get_features(model, os.path.join(MODEL_PATH, paths[i]), image_transforms[i], image))\n",
    "\n",
    "\n",
    "filenames = sorted(os.listdir(INPUT_PATH))\n",
    "\n",
    "breaker()\n",
    "for filename in filenames:\n",
    "    embeddings: list = []\n",
    "    css: list = []\n",
    "    for i in range(len(paths)):\n",
    "        image = get_image(os.path.join(INPUT_PATH, filename))\n",
    "        embeddings.append(get_features(model, os.path.join(MODEL_PATH, paths[i]), image_transforms[i], image))\n",
    "    for i in range(len(paths)): css.append(torch.nn.CosineSimilarity()(reference_embeddings[i], embeddings[i]).item())\n",
    "    print(f\"{filename} - CS1: {css[0]:.2f}, CS2: {css[1]:.2f}, CS3: {css[2]:.2f}, CS4: {css[3]:.2f}\")\n",
    "breaker()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 102.30752,
   "end_time": "2022-11-01T11:51:05.473975",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-11-01T11:49:23.166455",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
